{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def preprocess_csv(input_file, preprocessed_file):\n",
        "    with open(input_file, 'r') as infile:\n",
        "        reader = csv.reader(infile)\n",
        "\n",
        "        # Read and sort rows by frame number\n",
        "        header = next(reader)\n",
        "        rows = [row for row in reader]\n",
        "        rows.sort(key=lambda x: int(x[0]))\n",
        "\n",
        "    with open(preprocessed_file, 'w', newline='') as outfile:\n",
        "        writer = csv.writer(outfile)\n",
        "\n",
        "        # Write header\n",
        "        writer.writerow(header)\n",
        "\n",
        "        # Write sorted and corrected rows\n",
        "        for row in rows:\n",
        "            # Check and correct size category\n",
        "            if row[6] == '03-Jun':\n",
        "                row[6] = '3-6'\n",
        "\n",
        "            writer.writerow(row)"
      ],
      "metadata": {
        "id": "9hwkbxkanxXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bBcX_2cSoNIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_NEvy9gEKPo"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def process_csv(input_file, output_file, park, cam, date, fps=25):\n",
        "    window = 750  # 30 seconds window\n",
        "    max_distance = 120  # max distance to consider the same group\n",
        "\n",
        "    # Define the Group class to store group properties\n",
        "    class Group:\n",
        "        def __init__(self, first_frame, time, size_category, segment, radius, center_x, center_y):\n",
        "            self.first_frame = first_frame\n",
        "            self.time = time\n",
        "            self.size_category = size_category\n",
        "            self.segment = segment\n",
        "            self.radius = radius\n",
        "            self.center_x = center_x\n",
        "            self.center_y = center_y\n",
        "            self.detections = 1\n",
        "            self.frames_active = 1\n",
        "            self.active = True\n",
        "\n",
        "    # Read the input CSV and store rows\n",
        "    with open(input_file, 'r') as infile:\n",
        "        reader = csv.reader(infile)\n",
        "        header = next(reader)\n",
        "        rows = [row for row in reader]\n",
        "\n",
        "    active_groups = []\n",
        "    frame_groups = {}\n",
        "\n",
        "    for row in rows:\n",
        "        frame = int(row[0])\n",
        "        time = row[1]\n",
        "        center_x = float(row[2])\n",
        "        center_y = float(row[3])\n",
        "        radius = float(row[4])\n",
        "        size = int(row[5])\n",
        "        size_category = row[6]\n",
        "        segment = row[7]\n",
        "\n",
        "        if frame not in frame_groups:\n",
        "            frame_groups[frame] = []\n",
        "        frame_groups[frame].append((frame, time, center_x, center_y, radius, size, size_category, segment))\n",
        "\n",
        "    # Process frames\n",
        "    for frame in sorted(frame_groups.keys()):\n",
        "        current_groups = frame_groups[frame]\n",
        "\n",
        "        # Update existing groups\n",
        "        for group in active_groups:\n",
        "            group.frames_active += 1\n",
        "\n",
        "        for current_group in current_groups:\n",
        "            frame, time, center_x, center_y, radius, size, size_category, segment = current_group\n",
        "\n",
        "            matched = False\n",
        "            for group in active_groups:\n",
        "                if group.active and group.size_category == size_category and group.segment == segment:\n",
        "                    distance = ((group.center_x - center_x)**2 + (group.center_y - center_y)**2)**0.5\n",
        "                    if distance <= max(1.5*group.radius, 150):\n",
        "                        group.center_x = center_x\n",
        "                        group.center_y = center_y\n",
        "                        group.radius = radius\n",
        "                        group.detections += 1\n",
        "                        group.frames_active = 0  # Reset frames_active as it got a new detection\n",
        "                        matched = True\n",
        "                        break\n",
        "\n",
        "            if not matched:\n",
        "                new_group = Group(frame, time, size_category, segment, radius, center_x, center_y)\n",
        "                active_groups.append(new_group)\n",
        "\n",
        "        # Deactivate groups that haven't been updated in the window period\n",
        "        for group in active_groups:\n",
        "            if group.frames_active > window and group.active:\n",
        "                group.active = False\n",
        "\n",
        "    # Filter and write stable groups to the output CSV\n",
        "    with open(output_file, 'w', newline='') as outfile:\n",
        "        writer = csv.writer(outfile)\n",
        "        writer.writerow(['First Frame', 'Detection Time', 'Segment', 'Center X', 'Center Y', 'Radius', 'Size Category', 'Duration', 'Park', 'Camera', 'Date'])\n",
        "\n",
        "        for group in active_groups:\n",
        "            if group.detections >= 4:  # Consider only groups with at least 4 detections\n",
        "                duration = group.frames_active / fps\n",
        "                if duration >= 10: # Consider only groups that existed for more than 10 seconds\n",
        "                  writer.writerow([group.first_frame, group.time , group.segment , group.center_x, group.center_y, group.radius, group.size_category, duration, park, cam, date])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "park = \"Remez\"\n",
        "cam = \"1\"\n",
        "date = \"2304\"\n",
        "#input_file = '/content/raw_groups_remez_' + date + '_' + time + '_' + cam + '.csv'\n",
        "input_file = '/content/raw_groups_remez_' + date + '_' + cam + '.csv'\n",
        "#preprocessed_file = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/' + time + '/cam' + cam + '/raw_groups_remez_' + date + '_' + time + '_' + cam + '.csv'\n",
        "#output_file = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/' + time + '/cam' + cam + '/stable_groups_remez_' + date + '_' + time + '_' + cam + '.csv'\n",
        "#summary_file = '/content/drive/MyDrive/GroupSizeProject/' + park + '/' + date + '/' + time + '/cam' + cam + '/size_category_counts_remez_' + date + '_' + time + '_' + cam + '.csv'\n",
        "preprocessed_file = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/combined/cam' + cam + '/raw_groups_remez_' + date + '_' + cam + '_combined.csv'\n",
        "output_file = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/combined/cam' + cam + '/stable_groups_remez_' + date + '_' + cam + '_combined_new.csv'\n",
        "summary_file = '/content/drive/MyDrive/GroupSizeProject/' + park + '/' + date + '/combined/cam' + cam + '/size_category_counts_remez_' + date + '_' + cam + '_combined.csv'\n",
        "\n",
        "\n",
        "preprocess_csv(input_file, preprocessed_file)\n",
        "process_csv(preprocessed_file, output_file, park, cam, date)"
      ],
      "metadata": {
        "id": "yWfIciwhnvGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8dFM0rY5Yhb",
        "outputId": "a6c5585b-5b3b-41d6-c665-d46d06bde39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "oWnRrD_waWCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date = '2304'\n",
        "park = 'Katsenelson'\n",
        "time1 = '0812'\n",
        "time2 = '1216'\n",
        "time3 = '1620'\n",
        "cam = '2'\n",
        "file1 = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/' + time1 + '/cam' + cam + '/stable_groups_remez_' + date + '_' + time1 + '_' + cam + '.csv'\n",
        "file2 = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/' + time2 + '/cam' + cam + '/stable_groups_remez_' + date + '_' + time2 + '_' + cam + '.csv'\n",
        "file3 = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/' + time3 + '/cam' + cam + '/stable_groups_remez_' + date + '_' + time3 + '_' + cam + '.csv'\n",
        "output_file = '/content/drive/MyDrive/GroupSizeProject/'  + park + '/' + date + '/combined/cam' + cam + '/stable_groups_remez_' + date + '_' + cam + '_combined.csv'\n",
        "\n",
        "# Function to read and clean a CSV file\n",
        "def read_and_clean_csv(file_path, skip_first_row=False):\n",
        "    if skip_first_row:\n",
        "        df = pd.read_csv(file_path, skiprows=1)\n",
        "    else:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "    # Strip leading/trailing spaces from all string columns\n",
        "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "    return df\n",
        "\n",
        "# Read and clean the CSV files\n",
        "df1 = read_and_clean_csv(file1)\n",
        "df2 = read_and_clean_csv(file2, skip_first_row=True)\n",
        "df3 = read_and_clean_csv(file3, skip_first_row=True)\n",
        "\n",
        "# Combine the dataframes\n",
        "combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "# Remove any rows with all NaN values\n",
        "cleaned_combined_df = combined_df.dropna(how='all')\n",
        "\n",
        "# Save the cleaned combined dataframe to a new CSV file\n",
        "cleaned_combined_df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "U9DnRciAwDXu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}